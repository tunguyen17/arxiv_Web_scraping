{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"alg-geom\" : Algebraic Geometry (math.AG) \n",
    "\n",
    "\"dg-ga\" : Differential Geometry (math.DG)\n",
    "\n",
    "\"q-alg\" : Quantum Algebra (math.QA)\n",
    "\n",
    "\"patt-sol\" : Pattern Formation and Solitons (nlin.PS); \n",
    "\n",
    "\"adap-org\" : Adaptation and Self-Organizing Systems (nlin.AO)\n",
    "\n",
    "\"solv-int\" : Exactly Solvable and Integrable Systems (nlin.SI) \n",
    "\n",
    "\"chao-dyn\" : Chaotic Dynamics (nlin.CD) \n",
    "\n",
    "\"comp-gas\" : Cellular Automata and Lattice Gases (nlin.CG) \n",
    "\n",
    "\"chem-ph\" : Chemical Physics (physics.chem-ph) \n",
    "\n",
    "\"mtrl-th\" : Materials Science (cond-mat.mtrl-sci)\n",
    "\n",
    "\"cmp-lg\" : Computation and Language (cs.CL)\n",
    "\n",
    "\"atom-ph\" : Atomic Physics (physics.atom-ph) \n",
    "\n",
    "\"funct-an\" : Functional Analysis (math.FA)\n",
    "\n",
    "\"acc-phys\" : Accelerator Physics (physics.acc-ph) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "\n",
    "import urllib\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import io\n",
    "import re\n",
    "from math import floor\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finding all the categories\n",
    "\n",
    "url = \"http://export.arxiv.org/oai2?verb=ListSets\"\n",
    "u = urllib.request.urlopen(url, data = None)\n",
    "f = io.TextIOWrapper(u,encoding='utf-8')\n",
    "text = f.read()\n",
    "soup = BeautifulSoup(text, 'xml')\n",
    "all_cat = [sp.text for sp in soup.findAll(\"setSpec\")]\n",
    "\n",
    "f = open(\"all_cat_v01.txt\", \"w\")\n",
    "f.write(\",\".join(all_cat))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cs',\n",
       " 'econ',\n",
       " 'eess',\n",
       " 'math',\n",
       " 'physics',\n",
       " 'physics:astro-ph',\n",
       " 'physics:cond-mat',\n",
       " 'physics:gr-qc',\n",
       " 'physics:hep-ex',\n",
       " 'physics:hep-lat',\n",
       " 'physics:hep-ph',\n",
       " 'physics:hep-th',\n",
       " 'physics:math-ph',\n",
       " 'physics:nlin',\n",
       " 'physics:nucl-ex',\n",
       " 'physics:nucl-th',\n",
       " 'physics:physics',\n",
       " 'physics:quant-ph',\n",
       " 'q-bio',\n",
       " 'q-fin',\n",
       " 'stat']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(cat):\n",
    "    \n",
    "    # Initialization\n",
    "    df = pd.DataFrame(columns=(\"doi\", \"date\", \"title\", \"authors\", \"category\"))\n",
    "    base_url = \"http://export.arxiv.org/oai2?verb=ListRecords&\"\n",
    "    url = base_url + \"set={}&metadataPrefix=arXiv\".format(cat)\n",
    "    \n",
    "    # while loop in order to loop through all the resutls\n",
    "    while True:\n",
    "        # print url to keep track of stuff\n",
    "        print(url)\n",
    "        # accessing the url\n",
    "        try:\n",
    "            u = urllib.request.urlopen(url, data = None)\n",
    "        except HTTPError as e:\n",
    "            # Incase of some error that require us to wait\n",
    "            if e.code == 503:\n",
    "                to = int(e.hdrs.get(\"retry-after\", 30))\n",
    "                print(\"Got 503. Retrying after {0:d} seconds.\".format(to))\n",
    "                time.sleep(to)\n",
    "                continue # Skip this loop, continue to the next one\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        # reading the file\n",
    "        f = io.TextIOWrapper(u,encoding='utf-8')\n",
    "        text = f.read()\n",
    "        soup = BeautifulSoup(text, 'xml')\n",
    "\n",
    "        # collecting the data\n",
    "        for record in soup.findAll(\"record\"):\n",
    "            try:\n",
    "                doi = record.find(\"identifier\").text\n",
    "            except:\n",
    "                doi = np.nan\n",
    "            \n",
    "            try:\n",
    "                date = record.find(\"created\").text\n",
    "            except:\n",
    "                date = np.nan\n",
    "            \n",
    "            try:\n",
    "                title = record.find(\"title\").text\n",
    "            except:\n",
    "                title = np.nan\n",
    "            \n",
    "            try:\n",
    "                authors = \";\".join([author.get_text(\" \") for author in record.findAll(\"author\")])\n",
    "            except:\n",
    "                authros = np.nan\n",
    "            \n",
    "            try:\n",
    "                category = record.find(\"setSpec\").text\n",
    "            except:\n",
    "                category = np.nan\n",
    "                \n",
    "                df = df.append({\"doi\":doi, \"date\":date, \"title\":title, \"authors\":authors, \"category\":category}, ignore_index=True)\n",
    "                \n",
    "\n",
    "        # Seeing if there is still data\n",
    "\n",
    "        token = soup.find(\"resumptionToken\")\n",
    "        if token is None or token.text is None:\n",
    "            break\n",
    "        else:\n",
    "            url = base_url + \"resumptionToken=%s\"%(token.text)\n",
    "        \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- cs -------------------\n",
      "http://export.arxiv.org/oai2?verb=ListRecords&set=cs&metadataPrefix=arXiv\n"
     ]
    }
   ],
   "source": [
    "master_df = pd.DataFrame(columns=(\"doi\", \"date\", \"title\", \"authors\", \"category\"))\n",
    "for i in all_cat:\n",
    "    print(\"----------------\",i,\"-------------------\")\n",
    "    df = scrape(i)\n",
    "    master_df = master_df.append(df, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "master_df.to_csv(\"data_v3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
